{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c770065-f66e-4cec-93df-1a602d4d92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f05e20f-c350-4578-ba86-71f137a8dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34152/269612671.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/mouli/miniconda3/envs/gastro/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "#loading the embedding model from huggingface\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name,\n",
    "  model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab83409d-87bb-49b7-afe6-eb11f8317fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from local storage\n",
    "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e50012e-2dcb-45dc-8af3-f74c9d3a9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a retriever on top of database\n",
    "retriever = persisted_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea54b5b-62f1-4089-9f81-06ad39119807",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_template = \"\"\"\n",
    "        Task: You are an assistant that generates multiple variations of a given question. \n",
    "        For each variation, maintain the original intent of the question, but change the phrasing, structure, \n",
    "        or tone to create a diverse set of queries.\n",
    "\n",
    "Generate 5-7 variations that cover:\n",
    "\n",
    "Synonym replacements while keeping the question concise.\n",
    "Alternative structures, such as rephrasing into \"why,\" \"how,\" or \"what\" forms if relevant.\n",
    "Casual and formal tones.\n",
    "Slightly more specific or broader wording.\n",
    "Examples:\n",
    "\n",
    "Original Question: \"What is the impact of inflation on the stock market?\"\n",
    "Variations:\n",
    "\"How does inflation affect stock prices?\"\n",
    "\"What are the effects of inflation on the stock market?\"\n",
    "\"In what ways does inflation influence stock market trends?\"\n",
    "\"Could inflation lead to changes in stock market values?\"\n",
    "\"How does rising inflation impact the performance of stocks?\"\n",
    "\"What influence does inflation have on market prices?\"\n",
    "\"What happens to stock prices when inflation increases?\"\n",
    "\n",
    "the final answer should be a python list: [\n",
    "   \"How does inflation affect stock prices?\",\n",
    "    \"What are the effects of inflation on the stock market?\",\n",
    "    \"In what ways does inflation influence stock market trends?\",\n",
    "    \"Could inflation lead to changes in stock market values?\",\n",
    "    \"How does rising inflation impact the performance of stocks?\",\n",
    "    \"What influence does inflation have on market prices?\",\n",
    "    \"What happens to stock prices when inflation increases?\"\n",
    "]\n",
    "\n",
    "Now generate the list of variations for the given question\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db9c64c-757e-469f-9c61-73892b081cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = \"What are the new AI regulations in Europe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788485f9-d8cb-4267-ada3-0de2bbd189ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(input_variables=['original_query'],\n",
    "                            messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[],template=fusion_template)),\n",
    "                            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (7 queries):'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8770c828-8042-407b-b934-7eaf89d5df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff9211c-021e-4173-836d-4a574b820911",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b029c2d-bd2d-4be6-be5a-98dd35f7483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        Task: You are an assistant that generates multiple variations of a given question. \\n        For each variation, maintain the original intent of the question, but change the phrasing, structure, \\n        or tone to create a diverse set of queries.\\n\\nGenerate 5-7 variations that cover:\\n\\nSynonym replacements while keeping the question concise.\\nAlternative structures, such as rephrasing into \"why,\" \"how,\" or \"what\" forms if relevant.\\nCasual and formal tones.\\nSlightly more specific or broader wording.\\nExamples:\\n\\nOriginal Question: \"What is the impact of inflation on the stock market?\"\\nVariations:\\n\"How does inflation affect stock prices?\"\\n\"What are the effects of inflation on the stock market?\"\\n\"In what ways does inflation influence stock market trends?\"\\n\"Could inflation lead to changes in stock market values?\"\\n\"How does rising inflation impact the performance of stocks?\"\\n\"What influence does inflation have on market prices?\"\\n\"What happens to stock prices when inflation increases?\"\\n\\nthe final answer should be a python list: [\\n   \"How does inflation affect stock prices?\",\\n    \"What are the effects of inflation on the stock market?\",\\n    \"In what ways does inflation influence stock market trends?\",\\n    \"Could inflation lead to changes in stock market values?\",\\n    \"How does rising inflation impact the performance of stocks?\",\\n    \"What influence does inflation have on market prices?\",\\n    \"What happens to stock prices when inflation increases?\"\\n]\\n\\nNow generate the list of variations for the given question\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Generate multiple search queries related to: {question} \\n OUTPUT (7 queries):'), additional_kwargs={})]) middle=[OllamaLLM(model='llama3.2'), StrOutputParser()] last=RunnableLambda(lambda x: x.split('\\n'))\n"
     ]
    }
   ],
   "source": [
    "print(generate_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73900e0-5a4f-45f9-9349-21ff9e6ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3176efee-633a-4fc4-8276-4a830444997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragfusion_chain = generate_queries | retriever.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c0cbb5-1b8b-4a1f-9270-bd141d3b6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c676c7-1616-4746-98f1-5f91680c0eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['question'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragfusion_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d341c-38e9-45c1-adee-36e86e33c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragfusion_chain.invoke({\"question\": original_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909df50c-3447-4128-8df5-80df17e47410",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "full_rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\": ragfusion_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a78a62-718f-4daf-9167-a2e0656bfb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'question': {'title': 'Question', 'type': 'string'},\n",
       "  'root': {'title': 'Root'}},\n",
       " 'required': ['question', 'root'],\n",
       " 'title': 'RunnableParallel<context,question>Input',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_rag_fusion_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb3f97-4dd8-4193-93b9-0c12841fcd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = full_rag_fusion_chain.invoke({\"question\": \"How long does it typically take for a gastric ulcer to heal?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "258ac542-9bc3-40c3-87a2-cbcf80a68a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided clinical guidelines, the healing time for gastric ulcers is not explicitly mentioned. However, the general treatment approach for gastric ulcers and other gastrointestinal bleeding conditions is discussed in the guidelines.\n",
      "\n",
      "For patients with gastric varices, endoscopic injection of N-butyl-2-cyanoacrylate is offered to control upper gastrointestinal bleeding (Section 1.5.5). The duration of this treatment is not specified, but it is likely that the patient will need ongoing monitoring and potential repeat treatments as needed.\n",
      "\n",
      "In general, the healing time for gastric ulcers can vary depending on several factors, such as the severity of the ulcer, the effectiveness of treatment, and individual patient response. However, with appropriate medical management, including proton pump inhibitors (PPIs) or other anti-secretory medications, gastric ulcers typically heal within a few weeks to months.\n",
      "\n",
      "In the absence of specific information in the provided guidelines, it is difficult to provide an exact healing time for gastric ulcers. However, it is generally recommended that patients with gastrointestinal bleeding due to gastric ulcers should be closely monitored and treated with PPIs or other anti-secretory medications until symptoms resolve and endoscopic evaluation confirms healing.\n",
      "\n",
      "In a typical clinical scenario, the following timeline can be expected:\n",
      "\n",
      "* Within 1-3 days: Symptoms of gastric ulcer bleeding may start to resolve as the patient is treated with PPIs or other anti-secretory medications.\n",
      "* 7-14 days: Endoscopic evaluation typically takes place to assess the status of the ulcer and determine if healing has occurred.\n",
      "* 4-6 weeks: The patient should have adequate healing and be discharged from hospital care, provided there are no signs of re-bleeding or complications.\n",
      "\n",
      "Please note that these timelines can vary depending on individual patient factors and specific clinical circumstances.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20231154-0d4b-429f-a85c-3767d0825e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
